{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqe9INFIZwSFbUR3btHKar",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "92ddcfd220f34941b451e8586fa0c2fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e075dbc9c4c94f8e9c4e89da0927f131",
              "IPY_MODEL_ca0e7ca68bca4b6ba8a324f244a6718a",
              "IPY_MODEL_74cfa7c0a8804ae397276e705546fd6a"
            ],
            "layout": "IPY_MODEL_9c0a773be2314b589f205876e511b915"
          }
        },
        "e075dbc9c4c94f8e9c4e89da0927f131": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ad94ddf53a9456dad889435940dd73f",
            "placeholder": "​",
            "style": "IPY_MODEL_4b6cbcb71da049e38a4df7af4bedef2a",
            "value": "Downloading (…)rocessor_config.json: 100%"
          }
        },
        "ca0e7ca68bca4b6ba8a324f244a6718a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd613405d6654b6a934d097729ce1b1b",
            "max": 159,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7e7c11116c74fa19d025865f760309c",
            "value": 159
          }
        },
        "74cfa7c0a8804ae397276e705546fd6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8569a68f4311497ba44856f4389a44d3",
            "placeholder": "​",
            "style": "IPY_MODEL_d212d42a89ed4a17aefb1d74f2d45ee6",
            "value": " 159/159 [00:00&lt;00:00, 4.25kB/s]"
          }
        },
        "9c0a773be2314b589f205876e511b915": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ad94ddf53a9456dad889435940dd73f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b6cbcb71da049e38a4df7af4bedef2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd613405d6654b6a934d097729ce1b1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7e7c11116c74fa19d025865f760309c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8569a68f4311497ba44856f4389a44d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d212d42a89ed4a17aefb1d74f2d45ee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oyinig/African-AI-Foundation/blob/main/Copy_of_ConvertFileToNum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjoJCpG-Mo5k",
        "outputId": "af30d8f7-6f6f-4e63-c15d-26145c57753e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.33.3-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXmLWrzG1Y3M"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import os\n",
        "import io\n",
        "from transformers import Wav2Vec2FeatureExtractor, AutoTokenizer\n",
        "import torchaudio\n",
        "from torchaudio.transforms import Resample\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert an image in the content folder to numbers"
      ],
      "metadata": {
        "id": "ondWhC6CLjpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def image_to_matrix():\n",
        "    image = Image.open('/content/sample_data/Images/image.jpeg')\n",
        "    # Convert the image to grayscale\n",
        "    image_gray = image.convert('L')\n",
        "    # Convert the grayscale image to a numpy array\n",
        "    matrix_grey = np.array(image_gray)\n",
        "    print(\"Grey\", matrix_grey)\n",
        "    matrix_coloured = np.array(image)\n",
        "    print(\"Coloured\", matrix_coloured)\n",
        "    print(\"Grey Shape\", matrix_grey.shape)\n",
        "    print(\"Coloured Shape\", matrix_coloured.shape)\n",
        "    np.savetxt(\"/content/image_data.csv\", matrix_grey)\n",
        "\n",
        "\n",
        "image_to_matrix()"
      ],
      "metadata": {
        "id": "b4Fj8CR81era",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1493f09-7f51-42c1-b0b4-0a4fa980f0d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grey [[255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " ...\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]]\n",
            "Coloured [[[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]]\n",
            "Grey Shape (213, 237)\n",
            "Coloured Shape (213, 237, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ", output\n",
        "import torch\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "from PIL import Image\n",
        "import urllib.request\n",
        "\n",
        "# Load the pre-trained ViT model and feature extractor\n",
        "model_name = \"google/vit-base-patch16-224-in21k\"\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
        "model = ViTForImageClassification.from_pretrained(model_name)\n",
        "output = None\n",
        "# Load and preprocess the image\n",
        "\n",
        "image_path = \"https://www-konga-com-res.cloudinary.com/w_700,f_auto,fl_lossy,dpr_2.0,q_auto/media/catalog/product/N/B/144458_1647450047.jpg\"\n",
        "urllib.request.urlretrieve(image_path,\"image.jpg\")\n",
        "image = Image.open(\"image.jpg\")\n",
        "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "inputs"
      ],
      "metadata": {
        "id": "hV9uCAqXZBLX",
        "outputId": "549bc1be-bbc7-448c-9623-083e090fede0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pixel_values': tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]]]])}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_audio_features():\n",
        "    audio_file = '/content/sample_data/sample-3s.mp3.wav'\n",
        "\n",
        "    waveform, original_sample_rate = torchaudio.load(audio_file)\n",
        "\n",
        "    # Resample the audio waveform to 16000 Hz\n",
        "    resample_transform = Resample(orig_freq=original_sample_rate, new_freq=16000)\n",
        "    resampled_waveform = resample_transform(waveform)\n",
        "\n",
        "    # Initialize Wav2Vec2 feature extractor\n",
        "    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "    # Extract audio features\n",
        "    features = feature_extractor(resampled_waveform, sampling_rate=16000, return_tensors=\"pt\")\n",
        "    np.save('/content/extracted_audio_features.npy', features.input_values.numpy())\n",
        "    print(features)\n",
        "    print(features.input_values.shape)\n",
        "\n",
        "extract_audio_features()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "92ddcfd220f34941b451e8586fa0c2fa",
            "e075dbc9c4c94f8e9c4e89da0927f131",
            "ca0e7ca68bca4b6ba8a324f244a6718a",
            "74cfa7c0a8804ae397276e705546fd6a",
            "9c0a773be2314b589f205876e511b915",
            "6ad94ddf53a9456dad889435940dd73f",
            "4b6cbcb71da049e38a4df7af4bedef2a",
            "bd613405d6654b6a934d097729ce1b1b",
            "d7e7c11116c74fa19d025865f760309c",
            "8569a68f4311497ba44856f4389a44d3",
            "d212d42a89ed4a17aefb1d74f2d45ee6"
          ]
        },
        "id": "1125UlrVKB9n",
        "outputId": "4ad2d39c-3999-427e-d8d4-f21f1b498e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)rocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92ddcfd220f34941b451e8586fa0c2fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_values': tensor([[[ 0.0057,  0.0009, -0.0051,  ..., -0.0022, -0.0030, -0.0029]]])}\n",
            "torch.Size([1, 1, 184960])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Converting text to number encoding/embedding**\n",
        "\n",
        "The numeric encoding carry the semantic meaning of the text\n",
        "\n",
        "The numbers are not random but based on the word-number matrix/mapping used by the model\n",
        "\n",
        "If you use a model tokenizer to extract features, you must use the model for other downstream tasks, because each model encodes their embeddings differently\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-0oysM2eayxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Input words - Try to make them the same no of words\n",
        "input_words1 = \"Convert these words to numbers. African AI Foundation is creating an army of AI champions\"\n",
        "\n",
        "\n",
        "\n",
        "# Tokenize the input words\n",
        "tokens = tokenizer(input_words1, return_tensors=\"pt\")\n",
        "\n",
        "# Convert tokens to numbers (token IDs)\n",
        "token_ids = tokens['input_ids']\n",
        "token_ids"
      ],
      "metadata": {
        "id": "C_LvnI3qZyRE",
        "outputId": "dfecd34c-7e18-400c-d340-167e82ba823a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  101, 10463,  2122,  2616,  2000,  3616,  1012,  3060,  9932,  3192,\n",
              "          2003,  4526,  2019,  2390,  1997,  9932,  3966,   102]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Input words - Try to make them the same no of words\n",
        "input_words1 = \"Convert these words to numbers. African AI Foundation is creating an army of AI champions\"\n",
        "input_words2 = \"Converted the words to numbers. African AI Foundation has created an army of AI champions\"\n",
        "input_words3 =  \"The African economy needs to grow for it to become very competitive like the European economy\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Tokenize the input words\n",
        "tokens1 = tokenizer(input_words1, return_tensors=\"pt\")\n",
        "tokens2 = tokenizer(input_words2, return_tensors=\"pt\")\n",
        "tokens3 = tokenizer(input_words3, return_tensors=\"pt\")\n",
        "\n",
        "# Convert tokens to numbers (token IDs)\n",
        "token_ids = tokens['input_ids']\n"
      ],
      "metadata": {
        "id": "36MKw4b3HmxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens3 = tokenizer(input_words3, return_tensors=\"pt\")\n",
        "print(\"\\n\\n token 1 token ids \", tokens1['input_ids'], \" \\n\\n token 2 token ids\", tokens2['input_ids'], \"\\n\\n input 3 token ids \", tokens2['input_ids'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9D88iOL0Yle",
        "outputId": "417fe3b2-e085-425d-c132-a4f63b018a76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " token 1 token ids  tensor([[  101, 10463,  2122,  2616,  2000,  3616,  1012,  3060,  9932,  3192,\n",
            "          2003,  4526,  2019,  2390,  1997,  9932,  3966,   102]])  \n",
            "\n",
            " token 2 token ids tensor([[ 101, 4991, 1996, 2616, 2000, 3616, 1012, 3060, 9932, 3192, 2038, 2580,\n",
            "         2019, 2390, 1997, 9932, 3966,  102]]) \n",
            "\n",
            " input 3 token ids  tensor([[ 101, 4991, 1996, 2616, 2000, 3616, 1012, 3060, 9932, 3192, 2038, 2580,\n",
            "         2019, 2390, 1997, 9932, 3966,  102]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cosine Similarity search between token 1 and 2. Higher number means very similar\n",
        "\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "cos_sim = dot(tokens1.input_ids[0], tokens2.input_ids[0])/(norm(tokens1.input_ids[0])*norm(tokens2.input_ids[0]))\n",
        "cos_sim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKR0vSSQfR3M",
        "outputId": "1786f0b4-0501-49db-b037-5a90433a992a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9622583131501405"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cosine Similarity search between token 2 and 3. Lower number means less similar\n",
        "\n",
        "cos_sim = dot(tokens2.input_ids[0], tokens3.input_ids[0])/(norm(tokens2.input_ids[0])*norm(tokens3.input_ids[0]))\n",
        "cos_sim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hw8bzgUVVJLG",
        "outputId": "4e4cceb1-c193-4c07-8ab7-42591be7e74d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6601614588771935"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2wazvA1qLhy-"
      }
    }
  ]
}